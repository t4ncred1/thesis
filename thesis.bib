
@article{quix_metadata_2016,
	title = {Metadata {Extraction} and {Management} in {Data} {Lakes} {With} {GEMMS}},
	copyright = {Copyright (c) 2016 Complex Systems Informatics and Modeling Quarterly},
	issn = {2255-9922},
	url = {https://csimq-journals.rtu.lv/article/view/csimq.2016-9.04},
	doi = {10.7250/csimq.2016-9.04},
	abstract = {In addition to volume and velocity, Big data is also characterized by its variety. Variety in structure and semantics requires new integration approaches which can resolve the integration challenges also for large volumes of data. Data lakes should reduce the upfront integration costs and provide a more flexible way for data integration and analysis, as source data is loaded in its original structure to the data lake repository. Some syntactic transformation might be applied to enable access to the data in one common repository; however, a deep semantic integration is done only after the initial loading of the data into the data lake. Thereby, data is easily made available and can be restructured, aggregated, and transformed as required by later applications. Metadata management is a crucial component in a data lake, as the source data needs to be described by metadata to capture its semantics. We developed a Generic and Extensible Metadata Management System for data lakes (called GEMMS) that aims at the automatic extraction of metadata from a wide variety of data sources. Furthermore, the metadata is managed in an extensible metamodel that distinguishes structural and semantical metadata. The use case applied for evaluation is from the life science domain where the data is often stored only in files which hinders data access and efficient querying. The GEMMS framework has been proven to be useful in this domain. Especially, the extensibility and flexibility of the framework are important, as data and metadata structures in scientific experiments cannot be defined a priori.},
	language = {en-US},
	number = {9},
	urldate = {2022-04-26},
	journal = {Complex Systems Informatics and Modeling Quarterly},
	author = {Quix, Christoph and Hai, Rihan and Vatov, Ivan},
	month = dec,
	year = {2016},
	note = {Number: 9},
	keywords = {data integration, data lakes, metadata extraction, Metadata management, scientific data},
	pages = {67--83},
	file = {quixMetadataExtractionManagement2016.pdf:/home/tancrausen/Zotero/storage/I6KGRKJE/quixMetadataExtractionManagement2016.pdf:application/pdf},
}

@inproceedings{scholly_houdal_2021,
	title = {{HOUDAL}: {A} {Data} {Lake} {Implemented} for {Public} {Housing}},
	shorttitle = {{HOUDAL}},
	doi = {10.5220/0010418200390050},
	abstract = {This paper presents a real use case on public housing and proposes a data lake framework that is versatile enough to meet the challenges induced by the use case, and presents HOUDAL, an implementation of a dataLake framework based on the framework, which is operational and used by a social landlord. Like all areas of economic activity, public housing is impacted by the rise of big data. While Business Intelligence and Data Science analyses are more or less mastered by social landlords, combining them inside a shared environment is still a challenge. Moreover, processing big data, such as geographical open data that sometimes exceed the capacity of traditional tools, raises a second issue. To face these problems, we propose to use a data lake, a system in which data of any type can be stored and from which various analyses can be performed. In this paper, we present a real use case on public housing that fueled our motivation to introduce a data lake. We also propose a data lake framework that is versatile enough to meet the challenges induced by the use case. Finally, we present HOUDAL, an implementation of a data lake based on our framework, which is operational and used by a social landlord.},
	booktitle = {{ICEIS}},
	author = {Scholly, Étienne and Favre, Cécile and Ferey, Éric and Loudcher, Sabine},
	year = {2021},
	file = {schollyHOUDALDataLake2021.pdf:/home/tancrausen/Zotero/storage/LR33VP9B/schollyHOUDALDataLake2021.pdf:application/pdf},
}

@inproceedings{scholly_business_2019,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Business {Intelligence} \& {Analytics} {Applied} to {Public} {Housing}},
	isbn = {978-3-030-30278-8},
	doi = {10.1007/978-3-030-30278-8_53},
	abstract = {Business Intelligence, with data warehouses, reporting and OnLine Analytical Processing (OLAP) are about twenty years old technologies, they are mastered and widely used in companies. Their goal is to collect, organize, store and analyse data to support decision-making. In parallel, there are many algorithms from Data Science for conducting advanced data analyses, including the ability to conduct predictive analyses. However, the reflection on the integration of Data Science methods into reporting or OLAP analysis is relatively incomplete, although there is a real demand from companies to integrate prediction into decision-making processes. In the meantime, with the rise of the Internet, the proliferation of multimedia data (sound, image, video, etc.), and the fast development of social networks, data has become massive, heterogeneous, of diverse and rapid varieties. The Big Data phenomenon challenges the process of data storage and analysis and creates new research problems.},
	language = {en},
	booktitle = {New {Trends} in {Databases} and {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Scholly, Étienne},
	editor = {Welzer, Tatjana and Eder, Johann and Podgorelec, Vili and Wrembel, Robert and Ivanović, Mirjana and Gamper, Johann and Morzy, Mikoƚaj and Tzouramanis, Theodoros and Darmont, Jérôme and Kamišalić Latifić, Aida},
	year = {2019},
	keywords = {Big data, Business intelligence, Data Science},
	pages = {552--557},
	file = {schollyBusinessIntelligenceAnalytics2019.pdf:/home/tancrausen/Zotero/storage/P3GMCKZE/schollyBusinessIntelligenceAnalytics2019.pdf:application/pdf},
}

@inproceedings{sawadogo_metadata_2019,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Metadata {Systems} for {Data} {Lakes}: {Models} and {Features}},
	isbn = {978-3-030-30278-8},
	shorttitle = {Metadata {Systems} for {Data} {Lakes}},
	doi = {10.1007/978-3-030-30278-8_43},
	abstract = {Over the past decade, the data lake concept has emerged as an alternative to data warehouses for storing and analyzing big data. A data lake allows storing data without any predefined schema. Therefore, data querying and analysis depend on a metadata system that must be efficient and comprehensive. However, metadata management in data lakes remains a current issue and the criteria for evaluating its effectiveness are more or less nonexistent.},
	language = {en},
	booktitle = {New {Trends} in {Databases} and {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Sawadogo, Pegdwendé N. and Scholly, Étienne and Favre, Cécile and Ferey, Éric and Loudcher, Sabine and Darmont, Jérôme},
	editor = {Welzer, Tatjana and Eder, Johann and Podgorelec, Vili and Wrembel, Robert and Ivanović, Mirjana and Gamper, Johann and Morzy, Mikoƚaj and Tzouramanis, Theodoros and Darmont, Jérôme and Kamišalić Latifić, Aida},
	year = {2019},
	keywords = {Metadata management, Data lakes, Metadata modeling},
	pages = {440--451},
	file = {sawadogoMetadataSystemsData2019.pdf:/home/tancrausen/Zotero/storage/9SKFLEH6/sawadogoMetadataSystemsData2019.pdf:application/pdf},
}

@inproceedings{farrugia_towards_2016,
	title = {Towards social network analytics for understanding and managing enterprise data lakes},
	doi = {10.1109/ASONAM.2016.7752393},
	abstract = {We have built a tool for inspecting and managing data lakes. The motivations for creating this tool are 1) schema discovery (determining links pertinent to solving a data analysis problem), 2) discovering high risk links in data schemas that give rise to Information Security problems and 3) discovering high value relationships enabling data asset curation. The tool works by extracting metadata from the Hive database on a shared-tenancy instance of Hadoop, which contained a multi-terabyte real-world data asset. We use this metadata to calculate a graph of the relationships between the entities based on column matching. This allows us to apply Social Network Analysis (SNA) techniques in order to discover meaningful properties of the accumulated data. For example to extract previously unknown relationships between data entities. The challenges and the agenda for future research are also provided.},
	booktitle = {2016 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining} ({ASONAM})},
	author = {Farrugia, Ashley and Claxton, Rob and Thompson, Simon},
	month = aug,
	year = {2016},
	keywords = {Big data, Big Data, Data Discovery, Data Lake, Data mining, Data Warehouse, Database, Databases, Lakes, Organizations, Privacy, Social Network Analysis, Social network services},
	pages = {1213--1220},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/W2LC7ULH/7752393.html:text/html},
}

@inproceedings{barbierato_performance_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Performance {Evaluation} of a {Data} {Lake} {Architecture} via {Modeling} {Techniques}},
	isbn = {978-3-030-91825-5},
	abstract = {Data Lake is a term denoting a repository storing heterogeneous data, both structured and unstructured, resulting in a flexible organization that allows Data Lake users to reorganize and integrate dynamically the information they need according to the required query or analysis. The success of its implementation depends on many factors, notably the distributed storage, the kind of media deployed, the data access protocols and the network used. However, flaws in the design might become evident only in a later phase of the system development, causing significant delays in complex projects. This article presents an application of queuing networks modeling technique to detect significant issues, such as bottlenecks and performance degradation, for different workload scenarios.},
	booktitle = {Performance {Engineering} and {Stochastic} {Modeling}},
	publisher = {Springer International Publishing},
	author = {Barbierato, Enrico and Gribaudo, Marco and Serazzi, Giuseppe and Tanca, Letizia},
	editor = {Ballarini, Paolo and Castel, Hind and Dimitriou, Ioannis and Iacono, Mauro and Phung-Duc, Tuan and Walraevens, Joris},
	year = {2021},
	keywords = {Data lake, JMT, Queuing networks},
	pages = {115--130},
	file = {barbieratoPerformanceEvaluationData2021.pdf:/home/tancrausen/Zotero/storage/2623ZXDM/barbieratoPerformanceEvaluationData2021.pdf:application/pdf},
}

@article{eder_data_2021,
	title = {Data quality for federated medical data lakes},
	volume = {ahead-of-print},
	doi = {10.1108/IJWIS-03-2021-0026},
	abstract = {Purpose
Medical research requires biological material and data collected through biobanks in reliable processes with quality assurance. Medical studies based on data with unknown or questionable quality are useless or even dangerous, as evidenced by recent examples of withdrawn studies. Medical data sets consist of highly sensitive personal data, which has to be protected carefully and is available for research only after the approval of ethics committees. The purpose of this research is to propose an architecture to support researchers to efficiently and effectively identify relevant collections of material and data with documented quality for their research projects while observing strict privacy rules.

Design/methodology/approach
Following a design science approach, this paper develops a conceptual model for capturing and relating metadata of medical data in biobanks to support medical research.

Findings
This study describes the landscape of biobanks as federated medical data lakes such as the collections of samples and their annotations in the European federation of biobanks (Biobanking and Biomolecular Resources Research Infrastructure – European Research Infrastructure Consortium, BBMRI-ERIC) and develops a conceptual model capturing schema information with quality annotation. This paper discusses the quality dimensions for data sets for medical research in-depth and proposes representations of both the metadata and data quality documentation with the aim to support researchers to effectively and efficiently identify suitable data sets for medical studies.

Originality/value
This novel conceptual model for metadata for medical data lakes has a unique focus on the high privacy requirements of the data sets contained in medical data lakes and also stands out in the detailed representation of data quality and metadata quality of medical data sets.},
	journal = {International Journal of Web Information Systems},
	author = {Eder, Johann and Shekhovtsov, Vladimir},
	month = jul,
	year = {2021},
	file = {ederDataQualityFederated2021.pdf:/home/tancrausen/Zotero/storage/GM4P3R7D/ederDataQualityFederated2021.pdf:application/pdf},
}

@incollection{sawadogo_joint_2021,
	title = {Joint {Management} and {Analysis} of {Textual} {Documents} and {Tabular} {Data} {Within} the {AUDAL} {Data} {Lake}},
	isbn = {978-3-030-82471-6},
	abstract = {In 2010, the concept of data lake emerged as an alternative to data warehouses for big data management. Data lakes follow a schema-on-read approach to provide rich and flexible analyses. However, although trendy in both the industry and academia, the concept of data lake is still maturing, and there are still few methodological approaches to data lake design. Thus, we introduce a new approach to design a data lake and propose an extensive metadata system to activate richer features than those usually supported in data lake approaches. We implement our approach in the AUDAL data lake, where we jointly exploit both textual documents and tabular data, in contrast with structured and/or semi-structured data typically processed in data lakes from the literature. Furthermore, we also innovate by leveraging metadata to activate both data retrieval and content analysis, including Text-OLAP and SQL querying. Finally, we show the feasibility of our approach using a real-word use case on the one hand, and a benchmark on the other hand.},
	author = {Sawadogo, Pegdwendé Nicolas and Darmont, Jérôme and Noûs, Camille},
	month = aug,
	year = {2021},
	doi = {10.1007/978-3-030-82472-3_8},
	pages = {88--101},
	file = {sawadogoJointManagementAnalysis2021.pdf:/home/tancrausen/Zotero/storage/4CL45N2Y/sawadogoJointManagementAnalysis2021.pdf:application/pdf},
}

@article{scholly_coining_nodate,
	title = {Coining {goldMEDAL}: {A} {New} {Contribution} {toData} {Lake} {Generic} {Metadata} {Modeling}},
	abstract = {The rise of big data has revolutionized data exploitation practices and led to the emergence of new concepts. Among them, data lakes have emerged as large heterogeneous data repositories that can be analyzed by various methods. An efficient data lake requires a metadata system that addresses the many problems arising when dealing with big data. In consequence, the study of data lake metadata models is currently an active research topic and many proposals have been made in this regard. However, existing metadata models are either tailored for a specific use case or insufficiently generic to manage different types of data lakes, including our previous model MEDAL. In this paper, we generalize MEDAL’s concepts in a new metadata model called goldMEDAL. Moreover, we compare goldMEDAL with the most recent state-of-the-art metadata models aiming at genericity and show that we can reproduce these metadata models with goldMEDAL’s concepts. As a proof of concept, we also illustrate that goldMEDAL allows the design of various data lakes by presenting three different use cases.},
	language = {en},
	author = {Scholly, Étienne and Sawadogo, Pegdwendé N and Liu, Pengfei and Espinosa-Oviedo, Javier A and Favre, Cécile and Loudcher, Sabine and Darmont, Jérôme and Noûs, Camille},
	pages = {10},
	file = {schollyCoiningGoldMEDALNew.pdf:/home/tancrausen/Zotero/storage/YEHRN8ZD/schollyCoiningGoldMEDALNew.pdf:application/pdf},
}

@inproceedings{ravat_top_keyword_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Top\_Keyword: {An} {Aggregation} {Function} for {Textual} {Document} {OLAP}},
	isbn = {978-3-540-85836-2},
	shorttitle = {Top\_Keyword},
	doi = {10.1007/978-3-540-85836-2_6},
	abstract = {For more than a decade, researches on OLAP and multidimensional databases have generated methodologies, tools and resource management systems for the analysis of numeric data. With the growing availability of digital documents, there is a need for incorporating text-rich documents within multidimensional databases as well as an adapted framework for their analysis. This paper presents a new aggregation function that aggregates textual data in an OLAP environment. The Top\_Keyword function (Top\_Kw for short) represents a set of documents by their most significant terms using a weighing function from information retrieval: tf.idf.},
	language = {en},
	booktitle = {Data {Warehousing} and {Knowledge} {Discovery}},
	publisher = {Springer},
	author = {Ravat, Franck and Teste, Olivier and Tournier, Ronan and Zurfluh, Gilles},
	editor = {Song, Il-Yeol and Eder, Johann and Nguyen, Tho Manh},
	year = {2008},
	keywords = {Aggregation function, Data warehouse, OLAP, Textual measure},
	pages = {55--64},
	file = {ravatTopKeywordAggregation2008.pdf:/home/tancrausen/Zotero/storage/54YH8A3Y/ravatTopKeywordAggregation2008.pdf:application/pdf},
}

@inproceedings{sawadogo_benchmarking_2021,
	address = {Cham},
	title = {Benchmarking {Data} {Lakes} {Featuring} {Structured} and {Unstructured} {Data} with {DLBench}},
	isbn = {978-3-030-86534-4},
	doi = {10.1007/978-3-030-86534-4_2},
	abstract = {In the last few years, the concept of data lake has become trendy for data storage and analysis. Thus, several approaches have been proposed to build data lake systems. However, these proposals are difficult to evaluate as there are no commonly shared criteria for comparing data lake systems. Thus, we introduce DLBench, a benchmark to evaluate and compare data lake implementations that support textual and/or tabular contents. More concretely, we propose a data model made of both textual and CSV documents, a workload model composed of a set of various tasks, as well as a set of performance-based metrics, all relevant to the context of data lakes. As a proof of concept, we use DLBench to evaluate an open source data lake system we previously developed.},
	language = {en},
	booktitle = {Big {Data} {Analytics} and {Knowledge} {Discovery}},
	publisher = {Springer International Publishing},
	author = {Sawadogo, Pegdwendé N. and Darmont, Jérôme},
	editor = {Golfarelli, Matteo and Wrembel, Robert and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
	year = {2021},
	pages = {15--26},
	file = {sawadogoBenchmarkingDataLakes2021.pdf:/home/tancrausen/Zotero/storage/PGSS9ZUZ/sawadogoBenchmarkingDataLakes2021.pdf:application/pdf},
}

@inproceedings{wang_mimic-extract_2020,
	address = {New York, NY, USA},
	series = {{CHIL} '20},
	title = {{MIMIC}-{Extract}: a data extraction, preprocessing, and representation pipeline for {MIMIC}-{III}},
	isbn = {978-1-4503-7046-2},
	shorttitle = {{MIMIC}-{Extract}},
	url = {https://doi.org/10.1145/3368555.3384469},
	doi = {10.1145/3368555.3384469},
	abstract = {Machine learning for healthcare researchers face challenges to progress and reproducibility due to a lack of standardized processing frameworks for public datasets. We present MIMIC-Extract, an open source pipeline for transforming the raw electronic health record (EHR) data of critical care patients from the publicly-available MIMIC-III database into data structures that are directly usable in common time-series prediction pipelines. MIMIC-Extract addresses three challenges in making complex EHR data accessible to the broader machine learning community. First, MIMIC-Extract transforms raw vital sign and laboratory measurements into usable hourly time series, performing essential steps such as unit conversion, outlier handling, and aggregation of semantically similar features to reduce missingness and improve robustness. Second, MIMIC-Extract extracts and makes prediction of clinically-relevant targets possible, including outcomes such as mortality and length-of-stay as well as comprehensive hourly intervention signals for ventilators, vasopressors, and fluid therapies. Finally, the pipeline emphasizes reproducibility and extensibility to future research questions. We demonstrate the pipeline's effectiveness by developing several benchmark tasks for outcome and intervention forecasting and assessing the performance of competitive models.},
	urldate = {2022-05-03},
	booktitle = {Proceedings of the {ACM} {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Shirly and McDermott, Matthew B. A. and Chauhan, Geeticka and Ghassemi, Marzyeh and Hughes, Michael C. and Naumann, Tristan},
	month = apr,
	year = {2020},
	keywords = {Healthcare, Machine learning, MIMIC-III, Reproducibility, Time series data},
	pages = {222--235},
	file = {wangMIMICExtractDataExtraction2020.pdf:/home/tancrausen/Zotero/storage/PXBRZC7Y/wangMIMICExtractDataExtraction2020.pdf:application/pdf},
}

@article{johnson_mimic-iii_2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201635},
	doi = {10.1038/sdata.2016.35},
	abstract = {MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	language = {en},
	number = {1},
	urldate = {2022-05-03},
	journal = {Scientific Data},
	author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	month = may,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Health care, Medical research, Outcomes research, Prognosis},
	pages = {160035},
	file = {johnsonMIMICIIIFreelyAccessible2016.pdf:/home/tancrausen/Zotero/storage/X4J8EHE8/johnsonMIMICIIIFreelyAccessible2016.pdf:application/pdf;Snapshot:/home/tancrausen/Zotero/storage/MF93PCLC/sdata201635.html:text/html},
}

@inproceedings{cerotti_throughput_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Throughput {Maximization} with {Multiclass} {Workloads} and {Resource} {Constraints}},
	isbn = {978-3-319-08219-6},
	doi = {10.1007/978-3-319-08219-6_17},
	abstract = {In this paper we study the impact of different types of constraints on the maximum throughput that a system can handle. In particular, we focus on constraints limiting the use of resources and/or the allowed response time. The problem is made even more difficult by the pronounced diversity in resource requirements of the different applications in execution, i.e., by the multiclass characteristic of the workloads. The proposed approach allows to determine the maximum load of the different classes, while still satisfying the considered performance objectives. An experimental validation of the described technique through the study of a realistic e-commerce application is presented.},
	language = {en},
	booktitle = {Analytical and {Stochastic} {Modeling} {Techniques} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Cerotti, Davide and Gribaudo, Marco and Krüger, Ingolf and Piazzolla, Pietro and Seracini, Filippo and Serazzi, Giuseppe},
	editor = {Sericola, Bruno and Telek, Miklós and Horváth, Gábor},
	year = {2014},
	keywords = {Admission Control, Arrival Rate, Linear Program Problem, Maximum Throughput, Queue Length},
	pages = {238--252},
	file = {cerottiThroughputMaximizationMulticlass2014.pdf:/home/tancrausen/Zotero/storage/W2J2MN4N/cerottiThroughputMaximizationMulticlass2014.pdf:application/pdf},
}

@article{fischer_markov-modulated_1993,
	title = {The {Markov}-modulated {Poisson} process ({MMPP}) cookbook},
	volume = {18},
	issn = {0166-5316},
	url = {https://www.sciencedirect.com/science/article/pii/016653169390035S},
	doi = {10.1016/0166-5316(93)90035-S},
	abstract = {Point processes whose arrival rates vary randomly over time arise in many applications of interest, notably in communications modeling. The Markov-modulated Poisson process has been extensively used for modeling these processes, because it qualitatively models the time-varying arrival rate and captures some of the important correlations between the interarrival times while still remaining analytically tractable. The purpose of this paper is to collect a number of useful results about Markov-modulated Poisson processes and queues with Markov-modulated input. It is intended for non-experts or for people who are familiar with the basic concepts and algorithms, but would like a summary of recent developments. Derivations and proofs have been intentionally omitted. They can be found in the publications listed at the end of each section. Many of the results and some of the text have been taken directly from the references, and the reader is encouraged to consult the relevant references for more detailed information.},
	language = {en},
	number = {2},
	urldate = {2022-08-26},
	journal = {Performance Evaluation},
	author = {Fischer, Wolfgang and Meier-Hellstern, Kathleen},
	month = sep,
	year = {1993},
	keywords = {Markov-modulated Poisson process, Phase-type distribution},
	pages = {149--171},
	file = {ScienceDirect Snapshot:/home/tancrausen/Zotero/storage/2G543KJL/016653169390035S.html:text/html},
}

@inproceedings{reinecke_hyperstar_2012,
	title = {{HyperStar}: {Phase}-{Type} {Fitting} {Made} {Easy}},
	shorttitle = {{HyperStar}},
	doi = {10.1109/QEST.2012.29},
	abstract = {We present HyperStar, an extensible tool with a graphical user-interface that enables simple and efficient fitting of phase-type distributions to data sets. The fitting process can be optimised using intuitive graphical selection of important features of the density. An extensible module interface enables application of the tool as a GUI for prototype implementations of new fitting algorithms.},
	booktitle = {2012 {Ninth} {International} {Conference} on {Quantitative} {Evaluation} of {Systems}},
	author = {Reinecke, Philipp and Krauß, Tilman and Wolter, Katinka},
	month = sep,
	year = {2012},
	keywords = {Clustering, Clustering algorithms, Computational modeling, Conferences, Fitting, Graphical user interfaces, Histograms, Object oriented modeling, Phase-type fitting, Simulation},
	pages = {201--202},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/UITJQY2H/6354649.html:text/html;reineckeHyperStarPhaseTypeFitting2012.pdf:/home/tancrausen/Zotero/storage/YLPKKAGY/reineckeHyperStarPhaseTypeFitting2012.pdf:application/pdf},
}

@article{calzarossa_workload_1993,
	title = {Workload characterization: a survey},
	volume = {81},
	issn = {1558-2256},
	shorttitle = {Workload characterization},
	doi = {10.1109/5.236191},
	abstract = {The performance of a system is determined by its characteristics as well as by the composition of the load being processed. Hence, its quantitative description is a fundamental part of all performance evaluation studies. Several methodologies for the construction of workload models, which are functions of the objective of the study, of the architecture of the system to be analyzed, and of the techniques adopted, are presented. A survey of a few applications of these methodologies to various types of systems (i.e., batch, interactive, database, network-based, parallel, supercomputer), is given.{\textless}{\textgreater}},
	number = {8},
	journal = {Proceedings of the IEEE},
	author = {Calzarossa, M. and Serazzi, G.},
	month = aug,
	year = {1993},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Application software, Councils, Database systems, Hardware, Independent component analysis, Multiprocessing systems, Performance analysis, Software performance, Supercomputers, Transaction databases},
	pages = {1136--1150},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/HVDBJ3L2/236191.html:text/html;IEEE Xplore Full Text PDF:/home/tancrausen/Zotero/storage/XKSK9KEL/Calzarossa e Serazzi - 1993 - Workload characterization a survey.pdf:application/pdf},
}

@phdthesis{parente_design_nodate,
	title = {The design of a data lake architecture for the healthcare use case : problems and solutions},
	url = {https://www.politesi.polimi.it/handle/10589/182663},
	urldate = {2022-10-21},
	author = {Parente, Sara},
	file = {The design of a data lake architecture for the hea.pdf:/home/tancrausen/Zotero/storage/WRTXQNZX/The design of a data lake architecture for the hea.pdf:application/pdf;The design of a data lake architecture for the healthcare use case \: problems and solutions:/home/tancrausen/Zotero/storage/CXDM7UFQ/182663.html:text/html},
}

@article{bladt_review_2005,
	title = {A {Review} on {Phase}-type {Distributions} and their {Use} in {Risk} {Theory}},
	volume = {35},
	issn = {0515-0361, 1783-1350},
	url = {https://www.cambridge.org/core/journals/astin-bulletin-journal-of-the-iaa/article/review-on-phasetype-distributions-and-their-use-in-risk-theory/3AA72038185F8A978F6FC201A438D9D3},
	doi = {10.1017/S0515036100014100},
	abstract = {Phase-type distributions, defined as the distributions of absorption times of certain Markov jump processes, constitute a class of distributions on the positive real axis which seems to strike a balance between generality and tractability. Indeed, any positive distribution may be approximated arbitrarily closely by phase-type distributions whereas exact solutions to many complex problems in stochastic modeling can be obtained either explicitly or numerically. In this paper we introduce phase-type distributions and retrieve some of their basic properties through appealing probabilistic arguments which, indeed, constitute their main feature of being mathematically tractable. This is illustrated in an example where we calculate the ruin probability for a rather general class of surplus processes where the premium rate is allowed to depend on the current reserve and where claims sizes are assumed to be of phase-type. Finally we discuss issues concerning statistical inference for phase-type distributions and related functionals such as e.g. a ruin probability.},
	language = {en},
	number = {1},
	urldate = {2022-10-25},
	journal = {ASTIN Bulletin: The Journal of the IAA},
	author = {Bladt, Mogens},
	month = may,
	year = {2005},
	note = {Publisher: Cambridge University Press},
	keywords = {Phase-type distribution, EM-algorithm, Markov chain Monte Carlo, ruin probability},
	pages = {145--161},
	file = {bladtReviewPhasetypeDistributions2005.pdf:/home/tancrausen/Zotero/storage/DV8IW29H/bladtReviewPhasetypeDistributions2005.pdf:application/pdf;Snapshot:/home/tancrausen/Zotero/storage/6AD47CF5/3AA72038185F8A978F6FC201A438D9D3.html:text/html},
}

@article{altiok_phase-type_1985,
	title = {On the {Phase}-{Type} {Approximations} of {General} {Distributions}},
	volume = {17},
	issn = {0740-817X},
	url = {https://doi.org/10.1080/07408178508975280},
	doi = {10.1080/07408178508975280},
	abstract = {In this paper the approximation of general distributions, with known squared coefficient of variation, by phase-type distributions using the first three moments is studied. The phase-type distributions presented in this paper can be fitted to observed data sets and also can be used to approximate general distributions to be used in analytical models as well as in simulation. Formulas for the approximating distributions are presented and examples are given to show the approximations.},
	number = {2},
	urldate = {2022-10-25},
	journal = {IIE Transactions},
	author = {Altiok, Tayfur},
	month = jun,
	year = {1985},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07408178508975280},
	pages = {110--116},
}

@article{cox_use_1955,
	title = {A use of complex probabilities in the theory of stochastic processes},
	volume = {51},
	issn = {1469-8064, 0305-0041},
	url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/use-of-complex-probabilities-in-the-theory-of-stochastic-processes/3DE2C9013903EDD218F5B85129F65B2C},
	doi = {10.1017/S0305004100030231},
	abstract = {The exponential distribution is very important in the theory of stochastic processes with discrete states in continuous time. A. K. Erlang suggested a method of extending to other distributions methods that apply in the first instance only to exponential distributions. His idea is generalized to cover all distributions with rational Laplace transforms; this involves the formal use of complex transition probabilities. Properties of the method are considered.},
	language = {en},
	number = {2},
	urldate = {2022-10-25},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Cox, D. R.},
	month = apr,
	year = {1955},
	note = {Publisher: Cambridge University Press},
	pages = {313--319},
	file = {pdf:/home/tancrausen/Zotero/storage/5CZYWP2F/pdf:application/pdf;Snapshot:/home/tancrausen/Zotero/storage/GYU7MAX7/3DE2C9013903EDD218F5B85129F65B2C.html:text/html},
}

@inproceedings{reinecke_phase-type_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Phase-{Type} {Fitting} {Using} {HyperStar}},
	isbn = {978-3-642-40725-3},
	doi = {10.1007/978-3-642-40725-3_13},
	abstract = {In this paper we provide a hands-on discussion of the use of the HyperStar phase-type fitting tool in common application scenarios. HyperStar allows fitting Hyper-Erlang distributions to empirical data, using a variety of algorithms and operation modes. We describe simple cluster-based fitting, a new graphical method for refining the density approximation, a new command-line interface, and the integration of HyperStar with a Mathematica implementation of a fitting algorithm. Furthermore, we describe the use of Hyper-Erlang distributions in simulation. Throughout our discussion we illustrate the concepts on a data set which has been shown to be difficult to fit with a PH distribution.},
	language = {en},
	booktitle = {Computer {Performance} {Engineering}},
	publisher = {Springer},
	author = {Reinecke, Philipp and Krauß, Tilman and Wolter, Katinka},
	editor = {Balsamo, Maria Simonetta and Knottenbelt, William J. and Marin, Andrea},
	year = {2013},
	keywords = {Phase-type fitting, Case-study, Tool description},
	pages = {164--175},
	file = {Reinecke et al. - 2013 - Phase-Type Fitting Using HyperStar.pdf:/home/tancrausen/Zotero/storage/3RI8JM9V/Reinecke et al. - 2013 - Phase-Type Fitting Using HyperStar.pdf:application/pdf},
}

@article{fraile_fitting_2005,
	title = {Fitting an {Exponential} {Distribution}},
	volume = {44},
	doi = {10.1175/JAM2271.1},
	abstract = {Exponential distributions of the type N = N0 exp(-lambdat) occur with a high frequency in a wide range of scientific disciplines. This paper argues against a widely spread method for calculating the lambda parameter in this distribution. When the ln function is applied to both members, the equation of a straight line in t is obtained, which may be fit by means of linear regression. However, the paper illustrates that this is equivalent to a least squares fit with a weight function that assigns more importance to the higher values of t. It is argued that the method of maximum likelihood should be applied, because it takes into account all of the data equally. An iterative method for determining lambda is proposed, based on the method of moments for cases in which only a truncated distribution is available.},
	journal = {Journal of Applied Meteorology - J APPL METEOROL},
	author = {Fraile, Roberto and García-Ortega, Eduardo},
	month = oct,
	year = {2005},
	pages = {1620--1625},
	file = {fraileFittingExponentialDistribution2005.pdf:/home/tancrausen/Zotero/storage/GVKD5CE3/fraileFittingExponentialDistribution2005.pdf:application/pdf},
}

@misc{noauthor_about_nodate,
	title = {About the {Unified} {Modeling} {Language} {Specification} {Version} 2.5.1},
	url = {https://www.omg.org/spec/UML/2.5.1},
	urldate = {2022-11-01},
	file = {About the Unified Modeling Language Specification Version 2.5.1:/home/tancrausen/Zotero/storage/8MHV3VJB/2.5.html:text/html},
}

@inproceedings{wu_overview_2018,
	title = {An overview on feature-based classification algorithms for multivariate time series},
	doi = {10.1109/ICCCBDA.2018.8386483},
	abstract = {The research on multivariate time series (MTS) has developed rapidly in the past two decades. As an important part of data mining, the classification task for MTS has gained increasing attention from experts of diverse fields. In this paper, 26 feature-based classification methods for MTS are analyzed and summarize. Since the extraction of temporal features are the core of feature-based MTS classification, these methods are mainly divided into two categories: methods with hand-crafted features and methods with learnt features. The principles and procedures of these methods are introduced, and the advantages and disadvantages are also analyzed. Besides, the recent research directions in MTS classification, such as: early classification, imbalanced classification and classification with missing value are also discussed.},
	booktitle = {2018 {IEEE} 3rd {International} {Conference} on {Cloud} {Computing} and {Big} {Data} {Analysis} ({ICCCBDA})},
	author = {Wu, Junfeng and Yao, Li and Liu, Bin},
	month = apr,
	year = {2018},
	keywords = {Classification algorithms, Correlation, Feature extraction, hand-crafted features, Hidden Markov models, learnt features, Matrix decomposition, multivariate time series, Time measurement, Time series analysis},
	pages = {32--38},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/X746B8TK/8386483.html:text/html;wuOverviewFeaturebasedClassification2018a.pdf:/home/tancrausen/Zotero/storage/G6MKQVEM/wuOverviewFeaturebasedClassification2018a.pdf:application/pdf},
}

@book{legato_principles_2004,
	title = {Principles of {Gender}-specific {Medicine}},
	isbn = {978-0-12-440907-1},
	abstract = {Principles of Gender-Specific Medicine examines how normal human biology differs between men and women and how the diagnosis and treatment of disease differs as a function of gender. This revealing research covers various conditions that predominantly occur in men, and as well conditions that predominantly occur in women. Among the subjects covered are cardiovascular disease, mood disorders, the immune system, lung cancer as a consequence of smoking, osteoporosis, diabetes, obesity, and infectious diseases. * Gathers important information in the field of gender-based biology and clinical medicine, proving that a patient's sex is increasingly important in preventing illness, making an accurate diagnosis, and choosing safe and effective treatment of disease * Addresses gender-specific areas ranging from organ transplantation, gall bladder and biliary diseases, to the epidemiology of osteoporosis and fractures in men and women * Many chapters present questions about future directions of investigations},
	language = {en},
	publisher = {Gulf Professional Publishing},
	author = {Legato, Marianne J. and Bilezikian, John P.},
	year = {2004},
	note = {Google-Books-ID: 53SeZ859TiQC},
}

@misc{johnson_alistair_mimic-iii_2015,
	title = {{MIMIC}-{III} {Clinical} {Database}},
	url = {https://physionet.org/content/mimiciii/1.4/},
	abstract = {MIMIC-III is a large, freely-available database comprising deidentified
health-related data associated with over forty thousand patients who stayed in
critical care units of the Beth Israel Deaconess Medical Center between 2001
and 2012. The database includes information such as demographics, vital sign
measurements made at the bedside ({\textasciitilde}1 data point per hour), laboratory test
results, procedures, medications, caregiver notes, imaging reports, and
mortality (including post-hospital discharge).

MIMIC supports a diverse range of analytic studies spanning epidemiology,
clinical decision-rule improvement, and electronic tool development. It is
notable for three factors: it is freely available to researchers worldwide; it
encompasses a diverse and very large population of ICU patients; and it
contains highly granular data, including vital signs, laboratory results, and
medications.},
	urldate = {2022-11-15},
	publisher = {PhysioNet},
	author = {Johnson, Alistair and Pollard, Tom and Mark, Roger},
	year = {2015},
	doi = {10.13026/C2XW26},
	note = {Version Number: 1.4
Type: dataset},
}

@incollection{dufour_chapter_2019,
	series = {Conceptual {Econometrics} {Using} {R}},
	title = {Chapter 1 - {Finite}-sample inference and nonstandard asymptotics with {Monte} {Carlo} tests and {R}},
	volume = {41},
	url = {https://www.sciencedirect.com/science/article/pii/S0169716119300367},
	abstract = {We review the concept of Monte Carlo test as a simulation-based inference procedure which allows one to construct tests with provably exact levels in situations where the distribution of a test statistic is difficult to establish but can be simulated. The number of simulations required can be extremely small, as low as 19 to run a test with level 0.05. We discuss three extensions of the method: (1) a randomized tie-breaking technique which allows one to use test statistics with discrete null distributions, without further information on the mass points; (2) an extension (maximized Monte Carlo tests) which yields provably valid tests when the test statistic depends on a (finite) number of nuisance parameters; (3) an asymptotic version which allows one to get asymptotically valid tests without any need to establish an asymptotic distribution. As the method is computer intensive, we describe an R package (MaxMC) that allows one to implement this type of procedure. A number of special cases and applications are discussed.},
	language = {en},
	urldate = {2022-11-15},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Dufour, Jean-Marie and Neves, Julien},
	editor = {Vinod, Hrishikesh D. and Rao, C. R.},
	month = jan,
	year = {2019},
	doi = {10.1016/bs.host.2019.05.001},
	keywords = {Autoregressive model, Behrens–Fisher, Bootstrap, Discrete distribution, Exact inference, Genetic algorithm, Kolmogorov–Smirnov, Maximized Monte Carlo, MMC, Monte Carlo test, Nonstandard asymptotic distribution, Particle swarm, R, Randomized tie-breaker, Simulated annealing, Singular Wald test, Test level, Test size},
	pages = {3--31},
	file = {ScienceDirect Snapshot:/home/tancrausen/Zotero/storage/KWS5JLB4/S0169716119300367.html:text/html},
}

@incollection{berger_kolmogorovsmirnov_2005,
	title = {Kolmogorov–{Smirnov} {Tests}},
	isbn = {978-0-470-01319-9},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470013192.bsa332},
	abstract = {While often confused, the Kolmogorov–Smirnov test and the Smirnov test are actually distinct. Specifically, the Kolmogorov–Smirnov test is used to test the goodness of fit of a given set of data to a theoretical distribution, making this a one-sample test. In contrast, the Smirnov test is a two-sample test, used to determine if two samples appear to follow the same distribution. The intuition behind the two tests is the same, however, in that both compare cumulative distribution functions, either two empirical cumulative distribution functions for the two-sample Smirnov test, or one empirical cumulative distribution function and one known cumulative distribution function for the one-sample Kolmogorov–Smirnov test.},
	language = {en},
	urldate = {2022-11-15},
	booktitle = {Encyclopedia of {Statistics} in {Behavioral} {Science}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Berger, Vance W. and Zhou, YanYan},
	year = {2005},
	doi = {10.1002/0470013192.bsa332},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470013192.bsa332},
	keywords = {distribution function, exact test, nonparametric test, ranks},
	file = {Snapshot:/home/tancrausen/Zotero/storage/FW62ZNNX/0470013192.html:text/html},
}

@incollection{harchol-balter_real-world_2013,
	address = {Cambridge},
	title = {Real-{World} {Workloads}: {High} {Variability} and {Heavy} {Tails}},
	isbn = {978-1-107-02750-3},
	shorttitle = {Real-{World} {Workloads}},
	url = {https://www.cambridge.org/core/books/performance-modeling-and-design-of-computer-systems/realworld-workloads-high-variability-and-heavy-tails/F649916BC333CC5E5FA2287891BD80B6},
	abstract = {Part VI discusses queueing analysis where the arrival process and/or service process are generally distributed.We start with Chapter 20, where we study empirical job size distributions from computing workloads. These are often characterized by heavy tails, very high variance, and decreasing failure rate. Importantly, these are very different from the Markovian (Exponential) distributions that have enabled the Markov-chain-based analysis that we have done so far.New distributions require new analysis techniques. The first of these, the method of phase-type distributions, is introduced in Chapter 21. Phase-type distributions allow us to represent general distributions as mixtures of Exponential distributions. This in turn enables the modeling of systems involving general distributions using Markov chains. However, the resulting Markov chains are very different from what we have seen before and often have no simple solution. We introduce matrix-analytic techniques for solving these chains numerically. Matrix-analytic techniques are very powerful. They are efficient and highly accurate. Unfortunately, they are still numerical techniques, meaning that they can only solve “instances” of the problem, rather than solving the problem symbolically in terms of the input variables.In Chapter 22 we consider a new setting: networks of Processor-Sharing (PS) servers with generally distributed job sizes. These represent networks of computers, where each computer time-shares among several jobs. We again exploit the idea of phasetype distributions to analyze these networks, proving the BCMP product form theorem for networks with PS servers. The BCMP theorem provides a simple closed-form solution for a very broad class of networks of PS servers.},
	urldate = {2022-11-17},
	booktitle = {Performance {Modeling} and {Design} of {Computer} {Systems}: {Queueing} {Theory} in {Action}},
	publisher = {Cambridge University Press},
	author = {Harchol-Balter, Mor},
	year = {2013},
	doi = {10.1017/CBO9781139226424.026},
	pages = {347--348},
	file = {Snapshot:/home/tancrausen/Zotero/storage/WTJHXF97/F649916BC333CC5E5FA2287891BD80B6.html:text/html},
}

@article{thummler_novel_2006,
	title = {A {Novel} {Approach} for {Phase}-{Type} {Fitting} with the {EM} {Algorithm}},
	volume = {3},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2006.27},
	abstract = {The representation of general distributions or measured data by phase-type distributions is an important and nontrivial task in analytical modeling. Although a large number of different methods for fitting parameters of phase-type distributions to data traces exist, many approaches lack efficiency and numerical stability. In this paper, a novel approach is presented that fits a restricted class of phase-type distributions, namely, mixtures of Erlang distributions, to trace data. For the parameter fitting, an algorithm of the expectation maximization type is developed. This paper shows that these choices result in a very efficient and numerically stable approach which yields phase-type approximations for a wide range of data traces that are as good or better than approximations computed with other less efficient and less stable fitting methods. To illustrate the effectiveness of the proposed fitting algorithm, we present comparative results for our approach and two other methods using six benchmark traces and two real traffic traces as well as quantitative results from queuing analysis},
	number = {3},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Thummler, A. and Buchholz, P. and Telek, M.},
	month = jul,
	year = {2006},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Algorithm design and analysis, Analytical models, communication networks, hyper-Erlang distributions., Markov processes, Numerical stability, Parameter estimation, Performance analysis and design aids, Phase measurement, Process design, Queueing analysis, Telecommunication traffic, Traffic control, traffic modeling},
	pages = {245--258},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/9UQXP98D/1673383.html:text/html;IEEE Xplore Full Text PDF:/home/tancrausen/Zotero/storage/76VSTCDG/Thummler et al. - 2006 - A Novel Approach for Phase-Type Fitting with the E.pdf:application/pdf},
}

@inproceedings{serazzi_java_2006,
	title = {Java {Modelling} {Tools}: an {Open} {Source} {Suite} for {Queueing} {Network} {Modelling} and {Workload} {Analysis}},
	shorttitle = {Java {Modelling} {Tools}},
	doi = {10.1109/QEST.2006.22},
	abstract = {The Java Modelling Tools (JMT) is an open source suite for performance evaluation, capacity planning and modelling of computer and communication systems. The suite implements numerous state-of-the-art algorithms for the exact, asymptotic and simulative analysis of queueing network models, either with or without product-form solution. Models can be described either through wizard dialogs or with a graphical user-friendly interface. The suite includes also a workload analysis tool based on clustering techniques.},
	booktitle = {Third {International} {Conference} on the {Quantitative} {Evaluation} of {Systems} - ({QEST}'06)},
	author = {Serazzi, G. and Casale, G. and Bertoli, M.},
	month = sep,
	year = {2006},
	pages = {119--120},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/NM97GYAA/1703997.html:text/html},
}

@book{lazowska_quantitative_1984,
	title = {Quantitative {System} {Performance}, {Computer} {System} {Analysis} {Using} {Queuing} {Network} {Models}},
	isbn = {978-0-13-746975-8},
	url = {libgen.li/file.php?md5=c644f89c441959afbda36bc14dbae60f},
	publisher = {Prentice Hall},
	author = {Lazowska, Edward D.},
	year = {1984},
}

@misc{noauthor_library_nodate,
	title = {Library {Genesis}},
	url = {https://libgen.rocks/ads.php?md5=c644f89c441959afbda36bc14dbae60f},
	urldate = {2022-11-17},
	file = {Library Genesis:/home/tancrausen/Zotero/storage/8HVUYZFS/ads.html:text/html},
}

@inproceedings{hanussek_bootable_2019,
	title = {{BOOTABLE}: {Bioinformatics} {Benchmark} {Tool} {Suite}},
	shorttitle = {{BOOTABLE}},
	doi = {10.1109/CCGRID.2019.00027},
	abstract = {The interest in analyzing biological data on a large scale has grown over the last years. Bioinformatic applications play an important role when it comes to the analysis of huge amounts of data. Due to the large amount of biological data and/or large problem spaces a considerable amount of computing resources is required to answer the raised research questions. In order to estimate which underlying hardware might be the most suitable for the bioinformatic tools applied, a well-defined benchmark suite is required. Such a benchmark suite can get useful in the case of purchasing hardware and even further for larger projects with the goal to establish a bioinformatics compute infrastructure. With this paper we present BOOTABLE, our bioinformatic benchmark suite. BOOTABLE currently contains six popular and widely used bioinformatic applications representing a broad spectrum of usage characteristics. It further includes an automated installation procedure and all required datasets. BOOTABLE is available from our Github repository (https://github.com/MaximilianHanussek/BOOTABLE) in various formats.},
	booktitle = {2019 19th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Grid} {Computing} ({CCGRID})},
	author = {Hanussek, Maximilian and Bartusch, Felix and Krüger, Jens and Kohlbacher, Oliver},
	month = may,
	year = {2019},
	keywords = {Hardware, Benchmark testing, benchmarks, bioinformatics, Bioinformatics, cloud computing, Genomics, high performance computing, Runtime, Tools},
	pages = {157--160},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/BTCTPBHX/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/tancrausen/Zotero/storage/XLSDLKLG/Hanussek et al. - 2019 - BOOTABLE Bioinformatics Benchmark Tool Suite.pdf:application/pdf},
}

@inproceedings{bhandarkar_adbench_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AdBench}: {A} {Complete} {Benchmark} for {Modern} {Data} {Pipelines}},
	isbn = {978-3-319-54334-5},
	shorttitle = {{AdBench}},
	doi = {10.1007/978-3-319-54334-5_8},
	abstract = {Since the introduction of Apache YARN, which modularly separated resource management and scheduling from the distributed programming frameworks, a multitude of YARN-native computation frameworks have been developed. These frameworks specialize in specific analytics variants. In addition to traditional batch-oriented computations (e.g. MapReduce, Apache Hive [14] and Apache Pig [18]), the Apache Hadoop ecosystem now contains streaming analytics frameworks (e.g. Apache Apex [8]), MPP SQL engines (e.g. Apache Trafodion [20], Apache Impala [15], and Apache HAWQ [12]), OLAP cubing frameworks (e.g. Apache Kylin [17]), frameworks suitable for iterative machine learning (e.g. Apache Spark [19] and Apache Flink [10]), and graph processing (e.g. GraphX). With emergence of Hadoop Distributed File System and its various implementations as preferred method of constructing a data lake, end-to-end data pipelines are increasingly being built on the Hadoop-based data lake platform.},
	language = {en},
	booktitle = {Performance {Evaluation} and {Benchmarking}. {Traditional} - {Big} {Data} - {Internet} of {Things}},
	publisher = {Springer International Publishing},
	author = {Bhandarkar, Milind},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	year = {2017},
	keywords = {Data Pipeline, Hadoop Distribute File System, Message Queue, Synthetic Data Generator, System Under Test},
	pages = {107--120},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/IMDP3F46/Bhandarkar - 2017 - AdBench A Complete Benchmark for Modern Data Pipe.pdf:application/pdf},
}

@inproceedings{wang_bigdatabench_2014,
	title = {{BigDataBench}: {A} big data benchmark suite from internet services},
	shorttitle = {{BigDataBench}},
	doi = {10.1109/HPCA.2014.6835958},
	abstract = {As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above. This paper presents our joint research efforts on this issue with several industrial partners. Our big data benchmark suite-BigDataBench not only covers broad application scenarios, but also includes diverse and representative data sets. Currently, we choose 19 big data benchmarks from dimensions of application scenarios, operations/ algorithms, data types, data sources, software stacks, and application types, and they are comprehensive for fairly measuring and evaluating big data systems and architecture. BigDataBench is publicly available from the project home page http://prof.ict.ac.cn/BigDataBench. Also, we comprehensively characterize 19 big data workloads included in BigDataBench with varying data inputs. On a typical state-of-practice processor, Intel Xeon E5645, we have the following observations: First, in comparison with the traditional benchmarks: including PARSEC, HPCC, and SPECCPU, big data applications have very low operation intensity, which measures the ratio of the total number of instructions divided by the total byte number of memory accesses; Second, the volume of data input has non-negligible impact on micro-architecture characteristics, which may impose challenges for simulation-based big data architecture research; Last but not least, corroborating the observations in CloudSuite and DCBench (which use smaller data inputs), we find that the numbers of L1 instruction cache (L1I) misses per 1000 instructions (in short, MPKI) of the big data applications are higher than in the traditional benchmarks; also, we find that L3 caches are effective for the big data applications, corroborating the observation in DCBench.},
	booktitle = {2014 {IEEE} 20th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Wang, Lei and Zhan, Jianfeng and Luo, Chunjie and Zhu, Yuqing and Yang, Qiang and He, Yongqiang and Gao, Wanling and Jia, Zhen and Shi, Yingjie and Zhang, Shujie and Zheng, Chen and Lu, Gang and Zhan, Kent and Li, Xiaona and Qiu, Bizhu},
	month = feb,
	year = {2014},
	note = {ISSN: 2378-203X},
	keywords = {Social network services, Benchmark testing, Computer architecture, Search engines, System software},
	pages = {488--499},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/P54ZL8EL/6835958.html:text/html;IEEE Xplore Full Text PDF:/home/tancrausen/Zotero/storage/QF8BEZLD/Wang et al. - 2014 - BigDataBench A big data benchmark suite from inte.pdf:application/pdf},
}

@inproceedings{han_big_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Big} {Data} {Benchmarking}},
	isbn = {978-3-319-13021-7},
	doi = {10.1007/978-3-319-13021-7_1},
	abstract = {Big data systems address the challenges of capturing, storing, managing, analyzing, and visualizing big data. Within this context, developing benchmarks to evaluate and compare big data systems has become an active topic for both research and industry communities. To date, most of the state-of-the-art big data benchmarks are designed for specific types of systems. Based on our experience, however, we argue that considering the complexity, diversity, and rapid evolution of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads. Given this motivation, in this paper, we first propose the key requirements and challenges in developing big data benchmarks from the perspectives of generating data with 4 V properties (i.e. volume, velocity, variety and veracity) of big data, as well as generating tests with comprehensive workloads for big data systems. We then present the methodology on big data benchmarking designed to address these challenges. Next, the state-of-the-art are summarized and compared, following by our vision for future research directions.},
	language = {en},
	booktitle = {Big {Data} {Benchmarks}, {Performance} {Optimization}, and {Emerging} {Hardware}},
	publisher = {Springer International Publishing},
	author = {Han, Rui and Lu, Xiaoyi and Xu, Jiangtao},
	editor = {Zhan, Jianfeng and Han, Rui and Weng, Chuliang},
	year = {2014},
	keywords = {Benchmark, Big data systems, Data, Tests},
	pages = {3--18},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/LMGGR9ZA/Han et al. - 2014 - On Big Data Benchmarking.pdf:application/pdf},
}

@misc{choi_generating_2018,
	title = {Generating {Multi}-label {Discrete} {Patient} {Records} using {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.06490},
	doi = {10.48550/arXiv.1703.06490},
	abstract = {Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
	month = jan,
	year = {2018},
	note = {arXiv:1703.06490 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted at Machine Learning in Health Care (MLHC) 2017},
	file = {arXiv.org Snapshot:/home/tancrausen/Zotero/storage/VVAQDG48/1703.html:text/html;choiGeneratingMultilabelDiscrete2018.pdf:/home/tancrausen/Zotero/storage/7DCKGJCI/choiGeneratingMultilabelDiscrete2018.pdf:application/pdf},
}

@book{dash_synthetic_2019,
	title = {Synthetic {Event} {Time} {Series} {Health} {Data} {Generation}},
	abstract = {Synthetic medical data which preserves privacy while maintaining utility can be used as an alternative to real medical data, which has privacy costs and resource constraints associated with it. At present, most models focus on generating cross-sectional health data which is not necessarily representative of real data. In reality, medical data is longitudinal in nature, with a single patient having multiple health events, non-uniformly distributed throughout their lifetime. These events are influenced by patient covariates such as comorbidities, age group, gender etc. as well as external temporal effects (e.g. flu season). While there exist seminal methods to model time series data, it becomes increasingly challenging to extend these methods to medical event time series data. Due to the complexity of the real data, in which each patient visit is an event, we transform the data by using summary statistics to characterize the events for a fixed set of time intervals, to facilitate analysis and interpretability. We then train a generative adversarial network to generate synthetic data. We demonstrate this approach by generating human sleep patterns, from a publicly available dataset. We empirically evaluate the generated data and show close univariate resemblance between synthetic and real data. However, we also demonstrate how stratification by covariates is required to gain a deeper understanding of synthetic data quality.},
	author = {Dash, Saloni and Dutta, Ritik and Guyon, Isabelle and Pavao, Adrien and Yale, Andrew and Bennett, Kristin},
	month = nov,
	year = {2019},
}

@article{creswell_generative_2018,
	title = {Generative {Adversarial} {Networks}: {An} {Overview}},
	volume = {35},
	issn = {1558-0792},
	shorttitle = {Generative {Adversarial} {Networks}},
	doi = {10.1109/MSP.2017.2765202},
	abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
	month = jan,
	year = {2018},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Machine learning, Convolutional codes, Data models, Generators, Image resolution, Semantics, Signal resolution, Training data},
	pages = {53--65},
	file = {creswellGenerativeAdversarialNetworks2018.pdf:/home/tancrausen/Zotero/storage/8RPZ9RBX/creswellGenerativeAdversarialNetworks2018.pdf:application/pdf;IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/8PF3N7J5/stamp.html:text/html},
}

@inproceedings{madera_next_2016,
	address = {New York, NY, USA},
	series = {{MEDES}},
	title = {The next information architecture evolution: the data lake wave},
	isbn = {978-1-4503-4267-4},
	shorttitle = {The next information architecture evolution},
	url = {https://doi.org/10.1145/3012071.3012077},
	doi = {10.1145/3012071.3012077},
	abstract = {Data warehouses and data marts have long been considered as the unique solution for providing end-users with decisional information. More recently, data lakes have been proposed in order to govern data swamps. However, no formal definition has been proposed in the literature. Existing works are not complete and miss important parts of the topic. In particular, they do not focus on the influence of the data gravity, the infrastructure role of those solutions and of course are proposing divergent definitions and positioning regarding the usage and the interaction with existing decision support system. In this paper, we propose a novel definition of data lakes, together with a comparison with other over several criteria as the way to populate them, how to use, what is the Data Lake end user profile. We claim that data lakes are complementary components in decisional information systems and we discuss their position and interactions regarding the other components by proposing an interaction model.},
	urldate = {2022-11-20},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Madera, Cedrine and Laurent, Anne},
	month = nov,
	year = {2016},
	keywords = {data lakes, data governance, data lab, data laboratory, data reservoirs, data warehouses, digital transformation, internet of things},
	pages = {174--180},
}

@article{wang_chestx-ray8_nodate,
	title = {{ChestX}-ray8: {Hospital}-{Scale} {Chest} {X}-{Ray} {Database} and {Benchmarks} on {Weakly}-{Supervised} {Classification} and {Localization} of {Common} {Thorax} {Diseases}},
	abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals’ Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.},
	language = {en},
	author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
	pages = {10},
	file = {Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf:/home/tancrausen/Zotero/storage/W5TDKCTL/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf:application/pdf},
}

@inproceedings{wang_chestx-ray8_2017,
	title = {{ChestX}-ray8: {Hospital}-scale {Chest} {X}-ray {Database} and {Benchmarks} on {Weakly}-{Supervised} {Classification} and {Localization} of {Common} {Thorax} {Diseases}},
	shorttitle = {{ChestX}-ray8},
	url = {http://arxiv.org/abs/1705.02315},
	doi = {10.1109/CVPR.2017.369},
	abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC},
	urldate = {2022-11-20},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
	month = jul,
	year = {2017},
	note = {arXiv:1705.02315 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	pages = {3462--3471},
	annote = {Comment: CVPR 2017 spotlight;V1: CVPR submission+supplementary; V2: Statistics and benchmark results on published ChestX-ray14 dataset are updated in Appendix B V3: Minor correction V4: new data download link upated: https://nihcc.app.box.com/v/ChestXray-NIHCC V5: Update benchmark results on the published data split in the appendix},
	file = {arXiv.org Snapshot:/home/tancrausen/Zotero/storage/ZYF9DWN6/1705.html:text/html;wangChestXray8HospitalscaleChest2017.pdf:/home/tancrausen/Zotero/storage/RT7SBR5G/wangChestXray8HospitalscaleChest2017.pdf:application/pdf},
}

@inproceedings{jann_modeling_1997,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Modeling of workload in {MPPs}},
	isbn = {978-3-540-69599-8},
	doi = {10.1007/3-540-63574-2_18},
	abstract = {In this paper we have characterized the inter-arrival time and service time distributions for jobs at a large MPP supercomputing center. Our findings show that the distributions are dispersive and complex enough that they require Hyper Erlang distributions to capture the first three moments of the observed workload. We also present the parameters from the characterization so that they can be easily used for both theoretical studies and the simulations of various scheduling algorithms.},
	language = {en},
	booktitle = {Job {Scheduling} {Strategies} for {Parallel} {Processing}},
	publisher = {Springer},
	author = {Jann, Joefon and Pattnaik, Pratap and Franke, Hubertus and Wang, Fang and Skovira, Joseph and Riordan, Joseph},
	editor = {Feitelson, Dror G. and Rudolph, Larry},
	year = {1997},
	keywords = {Erlang Distribution, Exponential Distribution, Phase Type Distribution, Service Time, Service Time Distribution},
	pages = {95--116},
	file = {jannModelingWorkloadMPPs1997.pdf:/home/tancrausen/Zotero/storage/4FAXAC4D/jannModelingWorkloadMPPs1997.pdf:application/pdf},
}

@inproceedings{kros_model-based_2017,
	title = {Model-{Based} {Performance} {Evaluation} of {Batch} and {Stream} {Applications} for {Big} {Data}},
	doi = {10.1109/MASCOTS.2017.21},
	abstract = {Batch and stream processing represent the two main approaches implemented by big data systems such as Apache Spark and Apache Flink. Although only stream applications are intended to satisfy real-time requirements, both approaches are required to meet certain response time constraints. In addition, cluster architectures continuously expand and computing resources constitute high investments and expenses for organizations. Therefore, planning required capacities and predicting response times is crucial. In this work, we present a performance modeling and simulation approach by using and extending the Palladio component model. We predict performance metrics of batch and stream applications and its underlying processing systems by the example of Apache Spark on Apache Hadoop. Whereas most related work concentrates on one specific processing technique and focuses on the metric response time, we propose a general approach and consider the utilization of resources as well. In different experiments we evaluated our approach using applications and data workloads of the HiBench benchmark suite. The results indicate accurate predictions for upscaling cluster sizes as well as workloads with errors less than 18\%.},
	booktitle = {2017 {IEEE} 25th {International} {Symposium} on {Modeling}, {Analysis}, and {Simulation} of {Computer} and {Telecommunication} {Systems} ({MASCOTS})},
	author = {Kroß, Johannes and Krcmar, Helmut},
	month = sep,
	year = {2017},
	note = {ISSN: 2375-0227},
	keywords = {Big Data, Computational modeling, Data models, big data, modeling, performance, simulation, Sparks, Time factors},
	pages = {80--86},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/P3KZ9F5N/stamp.html:text/html;krossModelBasedPerformanceEvaluation2017a.pdf:/home/tancrausen/Zotero/storage/LVDL5FES/krossModelBasedPerformanceEvaluation2017a.pdf:application/pdf},
}

@inproceedings{jia_characterizing_2014,
	title = {Characterizing and subsetting big data workloads},
	doi = {10.1109/IISWC.2014.6983058},
	abstract = {Big data benchmark suites must include a diversity of data and workloads to be useful in fairly evaluating big data systems and architectures. However, using truly comprehensive benchmarks poses great challenges for the architecture community. First, we need to thoroughly understand the behaviors of a variety of workloads. Second, our usual simulation-based research methods become prohibitively expensive for big data. As big data is an emerging field, more and more software stacks are being proposed to facilitate the development of big data applications, which aggravates these challenges. In this paper, we first use Principle Component Analysis (PCA) to identify the most important characteristics from 45 metrics to characterize big data workloads from BigDataBench, a comprehensive big data benchmark suite. Second, we apply a clustering technique to the principle components obtained from the PCA to investigate the similarity among big data workloads, and we verify the importance of including different software stacks for big data benchmarking. Third, we select seven representative big data workloads by removing redundant ones and release the BigDataBench simulation version, which is publicly available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Jia, Zhen and Zhan, Jianfeng and Wang, Lei and Han, Rui and McKee, Sally A. and Yang, Qiang and Luo, Chunjie and Li, Jingwei},
	month = oct,
	year = {2014},
	keywords = {Big data, Benchmark testing, Sparks, Couplings, Measurement, Microarchitecture, Software},
	pages = {191--201},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/TSXENGV7/6983058.html:text/html;jiaCharacterizingSubsettingBig2014a.pdf:/home/tancrausen/Zotero/storage/AX8GJTQD/jiaCharacterizingSubsettingBig2014a.pdf:application/pdf},
}

@inproceedings{jia_characterizing_2014-1,
	title = {Characterizing and subsetting big data workloads},
	doi = {10.1109/IISWC.2014.6983058},
	abstract = {Big data benchmark suites must include a diversity of data and workloads to be useful in fairly evaluating big data systems and architectures. However, using truly comprehensive benchmarks poses great challenges for the architecture community. First, we need to thoroughly understand the behaviors of a variety of workloads. Second, our usual simulation-based research methods become prohibitively expensive for big data. As big data is an emerging field, more and more software stacks are being proposed to facilitate the development of big data applications, which aggravates these challenges. In this paper, we first use Principle Component Analysis (PCA) to identify the most important characteristics from 45 metrics to characterize big data workloads from BigDataBench, a comprehensive big data benchmark suite. Second, we apply a clustering technique to the principle components obtained from the PCA to investigate the similarity among big data workloads, and we verify the importance of including different software stacks for big data benchmarking. Third, we select seven representative big data workloads by removing redundant ones and release the BigDataBench simulation version, which is publicly available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Jia, Zhen and Zhan, Jianfeng and Wang, Lei and Han, Rui and McKee, Sally A. and Yang, Qiang and Luo, Chunjie and Li, Jingwei},
	month = oct,
	year = {2014},
	keywords = {Big data, Benchmark testing, Sparks, Couplings, Measurement, Microarchitecture, Software},
	pages = {191--201},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/V9BKPY7Z/6983058.html:text/html;jiaCharacterizingSubsettingBig2014.pdf:/home/tancrausen/Zotero/storage/9RUE8BRI/jiaCharacterizingSubsettingBig2014.pdf:application/pdf},
}

@article{calzarossa_workload_2016,
	title = {Workload {Characterization}: {A} {Survey} {Revisited}},
	volume = {48},
	issn = {0360-0300},
	shorttitle = {Workload {Characterization}},
	url = {https://doi.org/10.1145/2856127},
	doi = {10.1145/2856127},
	abstract = {Workload characterization is a well-established discipline that plays a key role in many performance engineering studies. The large-scale social behavior inherent in the applications and services being deployed nowadays leads to rapid changes in workload intensity and characteristics and opens new challenging management and performance issues. A deep understanding of user behavior and workload properties and patterns is therefore compelling. This article presents a comprehensive survey of the state of the art of workload characterization by addressing its exploitation in some popular application domains. In particular, we focus on conventional web workloads as well as on the workloads associated with online social networks, video services, mobile apps, and cloud computing infrastructures. We discuss the peculiarities of these workloads and present the methodological approaches and modeling techniques applied for their characterization. The role of workload models in various scenarios (e.g., performance evaluation, capacity planning, content distribution, resource provisioning) is also analyzed.},
	number = {3},
	urldate = {2022-11-19},
	journal = {ACM Computing Surveys},
	author = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
	month = feb,
	year = {2016},
	keywords = {cloud computing, graph analysis, mobile apps, online social networks, performance evaluation, statistical techniques, user behavior, video services, web workload, Workload characterization, workload measurements},
	pages = {48:1--48:43},
	file = {calzarossaWorkloadCharacterizationSurvey2016.pdf:/home/tancrausen/Zotero/storage/FYFS5IWP/calzarossaWorkloadCharacterizationSurvey2016.pdf:application/pdf},
}

@inproceedings{bhandarkar_adbench_2017-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AdBench}: {A} {Complete} {Benchmark} for {Modern} {Data} {Pipelines}},
	isbn = {978-3-319-54334-5},
	shorttitle = {{AdBench}},
	doi = {10.1007/978-3-319-54334-5_8},
	abstract = {Since the introduction of Apache YARN, which modularly separated resource management and scheduling from the distributed programming frameworks, a multitude of YARN-native computation frameworks have been developed. These frameworks specialize in specific analytics variants. In addition to traditional batch-oriented computations (e.g. MapReduce, Apache Hive [14] and Apache Pig [18]), the Apache Hadoop ecosystem now contains streaming analytics frameworks (e.g. Apache Apex [8]), MPP SQL engines (e.g. Apache Trafodion [20], Apache Impala [15], and Apache HAWQ [12]), OLAP cubing frameworks (e.g. Apache Kylin [17]), frameworks suitable for iterative machine learning (e.g. Apache Spark [19] and Apache Flink [10]), and graph processing (e.g. GraphX). With emergence of Hadoop Distributed File System and its various implementations as preferred method of constructing a data lake, end-to-end data pipelines are increasingly being built on the Hadoop-based data lake platform.},
	language = {en},
	booktitle = {Performance {Evaluation} and {Benchmarking}. {Traditional} - {Big} {Data} - {Internet} of {Things}},
	publisher = {Springer International Publishing},
	author = {Bhandarkar, Milind},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	year = {2017},
	keywords = {Data Pipeline, Hadoop Distribute File System, Message Queue, Synthetic Data Generator, System Under Test},
	pages = {107--120},
	file = {bhandarkarAdBenchCompleteBenchmark2017a.pdf:/home/tancrausen/Zotero/storage/WD5T6XIC/bhandarkarAdBenchCompleteBenchmark2017a.pdf:application/pdf},
}

@book{dash_synthetic_2019-1,
	title = {Synthetic {Event} {Time} {Series} {Health} {Data} {Generation}},
	abstract = {Synthetic medical data which preserves privacy while maintaining utility can be used as an alternative to real medical data, which has privacy costs and resource constraints associated with it. At present, most models focus on generating cross-sectional health data which is not necessarily representative of real data. In reality, medical data is longitudinal in nature, with a single patient having multiple health events, non-uniformly distributed throughout their lifetime. These events are influenced by patient covariates such as comorbidities, age group, gender etc. as well as external temporal effects (e.g. flu season). While there exist seminal methods to model time series data, it becomes increasingly challenging to extend these methods to medical event time series data. Due to the complexity of the real data, in which each patient visit is an event, we transform the data by using summary statistics to characterize the events for a fixed set of time intervals, to facilitate analysis and interpretability. We then train a generative adversarial network to generate synthetic data. We demonstrate this approach by generating human sleep patterns, from a publicly available dataset. We empirically evaluate the generated data and show close univariate resemblance between synthetic and real data. However, we also demonstrate how stratification by covariates is required to gain a deeper understanding of synthetic data quality.},
	author = {Dash, Saloni and Dutta, Ritik and Guyon, Isabelle and Pavao, Adrien and Yale, Andrew and Bennett, Kristin},
	month = nov,
	year = {2019},
	file = {dashSyntheticEventTime2019.pdf:/home/tancrausen/Zotero/storage/SPU8YAUT/dashSyntheticEventTime2019.pdf:application/pdf},
}

@inproceedings{ming_bdgs_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{BDGS}: {A} {Scalable} {Big} {Data} {Generator} {Suite} in {Big} {Data} {Benchmarking}},
	isbn = {978-3-319-10596-3},
	shorttitle = {{BDGS}},
	doi = {10.1007/978-3-319-10596-3_11},
	abstract = {Data generation is a key issue in big data benchmarking that aims to generate application-specific data sets to meet the 4 V requirements of big data. Specifically, big data generators need to generate scalable data (Volume) of different types (Variety) under controllable generation rates (Velocity) while keeping the important characteristics of raw data (Veracity). This gives rise to various new challenges about how we design generators efficiently and successfully. To date, most existing techniques can only generate limited types of data and support specific big data systems such as Hadoop. Hence we develop a tool, called Big Data Generator Suite (BDGS), to efficiently generate scalable big data while employing data models derived from real data to preserve data veracity. The effectiveness of BDGS is demonstrated by developing six data generators covering three representative data types (structured, semi-structured and unstructured) and three data sources (text, graph, and table data).},
	language = {en},
	booktitle = {Advancing {Big} {Data} {Benchmarks}},
	publisher = {Springer International Publishing},
	author = {Ming, Zijian and Luo, Chunjie and Gao, Wanling and Han, Rui and Yang, Qiang and Wang, Lei and Zhan, Jianfeng},
	editor = {Rabl, Tilmann and Raghunath, Nambiar and Poess, Meikel and Bhandarkar, Milind and Jacobsen, Hans-Arno and Baru, Chaitanya},
	year = {2014},
	keywords = {Big data, Benchmark, Data generator, Scalable, Veracity},
	pages = {138--154},
	file = {mingBDGSScalableBig2014.pdf:/home/tancrausen/Zotero/storage/RQD4FLT7/mingBDGSScalableBig2014.pdf:application/pdf},
}

@misc{noauthor_introduction_nodate,
	title = {An {Introduction} to {Workload} {Characterization}},
	url = {https://support.novell.com/techcenter/articles/ana19910503.html},
	urldate = {2022-11-19},
	file = {An Introduction to Workload Characterization:/home/tancrausen/Zotero/storage/TCLRNWGA/ana19910503.html:text/html},
}

@article{yale_generation_2020,
	title = {Generation and evaluation of privacy preserving synthetic health data},
	volume = {416},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220305117},
	doi = {10.1016/j.neucom.2019.12.136},
	abstract = {We develop metrics for measuring the quality of synthetic health data for both education and research. We use novel and existing metrics to capture a synthetic dataset’s resemblance, privacy, utility and footprint. Using these metrics, we develop an end-to-end workflow based on our generative adversarial network (GAN) method, HealthGAN, that creates privacy preserving synthetic health data. Our workflow meets privacy specifications of our data partner: (1) the HealthGAN is trained inside a secure environment; (2) the HealthGAN model is used outside of the secure environment by external users to generate synthetic data. This second step facilitates data handling for external users by avoiding de-identification, which may require special user training, be costly, or cause loss of data fidelity. This workflow is compared against five other baseline methods. While maintaining resemblance and utility comparable to other methods, HealthGAN provides the best privacy and footprint. We present two case studies in which our methodology was put to work in the classroom and research settings. We evaluate utility in the classroom through a data analysis challenge given to students and in research by replicating three different medical papers with synthetic data. Data, code, and the challenge that we organized for educational purposes are available.},
	language = {en},
	urldate = {2022-11-19},
	journal = {Neurocomputing},
	author = {Yale, Andrew and Dash, Saloni and Dutta, Ritik and Guyon, Isabelle and Pavao, Adrien and Bennett, Kristin P.},
	month = nov,
	year = {2020},
	keywords = {Privacy, Generative adversarial networks, Health data, Synthetic data},
	pages = {244--255},
	file = {ScienceDirect Snapshot:/home/tancrausen/Zotero/storage/5LMKWBEG/S0925231220305117.html:text/html;yaleGenerationEvaluationPrivacy2020.pdf:/home/tancrausen/Zotero/storage/TDBTK28S/yaleGenerationEvaluationPrivacy2020.pdf:application/pdf},
}

@inproceedings{harris_bench4gis_2019,
	title = {bench4gis: {Benchmarking} {Privacy}-aware {Geocoding} with {Open} {Big} {Data}},
	shorttitle = {bench4gis},
	doi = {10.1109/BigData47090.2019.9006234},
	abstract = {Geocoding, the process of translating addresses to geographic coordinates, is a relatively straight-forward and well-studied process, but limitations due to privacy concerns may restrict usage of geographic data. The impact of these limitations are further compounded by the scale of the data, and in turn, also limits viable geocoding strategies. For example, healthcare data is protected by patient privacy laws in addition to possible institutional regulations that restrict external transmission and sharing of data. This results in the implementation of “in-house” geocoding solutions where data is processed behind an organization's firewall; quality assurance for these implementations is problematic because sensitive data cannot be used to externally validate results. In this paper, we present our software framework called bench4gis which benchmarks privacy-aware geocoding solutions by leveraging open big data as surrogate data for quality assurance; the scale of open big data sets for address data can ensure that results are geographically meaningful for the locale of the implementing institution.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Harris, Daniel R. and Delcher, Chris},
	month = dec,
	year = {2019},
	keywords = {Big Data, Benchmark testing, big data applications, Data privacy, geographic information systems, geospatial analysis, Geospatial analysis, Medical services, Open source software},
	pages = {4067--4070},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/M32J2V5U/9006234.html:text/html;IEEE Xplore Full Text PDF:/home/tancrausen/Zotero/storage/STPUXTMJ/Harris e Delcher - 2019 - bench4gis Benchmarking Privacy-aware Geocoding wi.pdf:application/pdf},
}

@inproceedings{mei_swim_2009,
	title = {{SWIM}: {A} {Simple} {Model} to {Generate} {Small} {Mobile} {Worlds}},
	shorttitle = {{SWIM}},
	doi = {10.1109/INFCOM.2009.5062134},
	abstract = {This paper presents small world in motion (SWIM), a new mobility model for ad-hoc networking. SWIM is relatively simple, is easily tuned by setting just a few parameters, and generates traces that look real-synthetic traces have the same statistical properties of real traces. SWIM shows experimentally and theoretically the presence of the power law and exponential decay dichotomy of inter-contact time, and, most importantly, our experiments show that it can predict very accurately the performance of forwarding protocols.},
	booktitle = {{IEEE} {INFOCOM} 2009},
	author = {Mei, A. and Stefa, J.},
	month = apr,
	year = {2009},
	note = {ISSN: 0743-166X},
	keywords = {Computational modeling, Communications Society, Computer science, Computer simulation, Humans, Mobile computing, Peer to peer computing, Power generation, Probability distribution, Protocols},
	pages = {2106--2113},
	file = {IEEE Xplore Abstract Record:/home/tancrausen/Zotero/storage/LTVWV5TP/5062134.html:text/html;IEEE Xplore Full Text PDF:/home/tancrausen/Zotero/storage/9LA4RJSL/Mei e Stefa - 2009 - SWIM A Simple Model to Generate Small Mobile Worl.pdf:application/pdf},
}

@article{sehgal_big_2018,
	title = {Big {Data}: {A} {Volume} or {Technology} ?},
	volume = {3},
	issn = {2278-0181},
	shorttitle = {Big {Data}},
	url = {https://www.ijert.org/research/big-data-a-volume-or-technology-IJERTCONV3IS10084.pdf, https://www.ijert.org/big-data-a-volume-or-technology},
	doi = {10.17577/IJERTCONV3IS10084},
	abstract = {Big Data: A Volume or Technology ? - written by Savita Sehgal , Yashpal Singh published on 2018/04/24 download full article with reference data and citations},
	language = {en-US},
	number = {10},
	urldate = {2022-11-22},
	journal = {International Journal of Engineering Research \& Technology},
	author = {Sehgal, Savita and Singh, Yashpal},
	month = apr,
	year = {2018},
	note = {Publisher: IJERT-International Journal of Engineering Research \& Technology},
	file = {sehgalBigDataVolume2018.pdf:/home/tancrausen/Zotero/storage/HU2CCQFQ/sehgalBigDataVolume2018.pdf:application/pdf;Snapshot:/home/tancrausen/Zotero/storage/GQGK2AXP/big-data-a-volume-or-technology.html:text/html},
}

@article{batko_use_2022,
	title = {The use of {Big} {Data} {Analytics} in healthcare},
	volume = {9},
	issn = {2196-1115},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8733917/},
	doi = {10.1186/s40537-021-00553-4},
	abstract = {The introduction of Big Data Analytics (BDA) in healthcare will allow to use new technologies both in treatment of patients and health management. The paper aims at analyzing the possibilities of using Big Data Analytics in healthcare. The research is based on a critical analysis of the literature, as well as the presentation of selected results of direct research on the use of Big Data Analytics in medical facilities. The direct research was carried out based on research questionnaire and conducted on a sample of 217 medical facilities in Poland. Literature studies have shown that the use of Big Data Analytics can bring many benefits to medical facilities, while direct research has shown that medical facilities in Poland are moving towards data-based healthcare because they use structured and unstructured data, reach for analytics in the administrative, business and clinical area. The research positively confirmed that medical facilities are working on both structural data and unstructured data. The following kinds and sources of data can be distinguished: from databases, transaction data, unstructured content of emails and documents, data from devices and sensors. However, the use of data from social media is lower as in their activity they reach for analytics, not only in the administrative and business but also in the clinical area. It clearly shows that the decisions made in medical facilities are highly data-driven. The results of the study confirm what has been analyzed in the literature that medical facilities are moving towards data-based healthcare, together with its benefits.},
	number = {1},
	urldate = {2022-11-22},
	journal = {Journal of Big Data},
	author = {Batko, Kornelia and Ślęzak, Andrzej},
	year = {2022},
	pmid = {35013701},
	pmcid = {PMC8733917},
	pages = {3},
	file = {batkoUseBigData2022.pdf:/home/tancrausen/Zotero/storage/CKA44JSC/batkoUseBigData2022.pdf:application/pdf},
}

@misc{moody_benjamin_mimic-iii_2017,
	title = {{MIMIC}-{III} {Waveform} {Database} {Matched} {Subset}},
	url = {https://physionet.org/content/mimic3wdb-matched/1.0/},
	abstract = {The MIMIC-III Waveform Database Matched Subset contains 22,317 waveform
records, and 22,247 numerics records, for 10,282 distinct ICU patients. These
recordings typically include digitized signals such as ECG, ABP, respiration,
and PPG, as well as periodic measurements such as heart rate, oxygen
saturation, and systolic, mean, and diastolic blood pressure.

This database is a subset of the [MIMIC-III Waveform
Database](/content/mimic3wdb/), representing those records for which the
patient has been identified, and their corresponding clinical records are
available in the [MIMIC-III Clinical Database](/content/mimiciii/).

  *[ICU]: Intensive care unit
  *[ECG]: Electrocardiogram
  *[ABP]: Arterial blood pressure
  *[PPG]: Photoplethysmogram},
	urldate = {2022-11-23},
	publisher = {PhysioNet},
	author = {Moody, Benjamin and Moody, George and Villarroel, Mauricio and Clifford, Gari and Silva, Ikaro},
	year = {2017},
	doi = {10.13026/C2294B},
	note = {Version Number: 1.0
Type: dataset},
}

@misc{moody_benjamin_mimic-iii_2017-1,
	title = {{MIMIC}-{III} {Waveform} {Database}},
	url = {https://physionet.org/content/mimic3wdb/1.0/},
	abstract = {The MIMIC-III Waveform Database contains 67,830 record sets for approximately
30,000 ICU patients. Almost all record sets include a waveform record
containing digitized signals (typically including ECG, ABP, respiration, and
PPG, and frequently other signals) and a "numerics" record containing time
series of periodic measurements, each presenting a quasi-continuous recording
of vital signs of a single patient throughout an ICU stay (typically a few
days, but many are several weeks in duration). A [subset of this
database](/content/mimic3wdb-matched/) contains waveform and numerics records
that have been matched and time-aligned with [MIMIC-III Clinical
Database](/content/mimiciii/) records.

  *[ICU]: Intensive care unit
  *[ECG]: Electrocardiogram
  *[ABP]: Arterial blood pressure
  *[PPG]: Photoplethysmogram},
	urldate = {2022-11-23},
	publisher = {PhysioNet},
	author = {Moody, Benjamin and Moody, George and Villarroel, Mauricio and Clifford, Gari and Silva, Ikaro},
	year = {2017},
	doi = {10.13026/C2607M},
	note = {Version Number: 1.0
Type: dataset},
}

@article{kadri_convolutions_2015,
	title = {Convolutions of hyper-erlang and of erlang distributions},
	volume = {98},
	number = {1},
	journal = {International Journal of Pure and Applied Mathematics},
	author = {Kadri, Therrar and Smaili, Khaled},
	year = {2015},
	pages = {81--98},
	file = {Full Text:/home/tancrausen/Zotero/storage/K4TXTZTX/Kadri and Smaili - 2015 - Convolutions of hyper-erlang and of erlang distrib.pdf:application/pdf},
}

@misc{noauthor_kolmogorovsmirnov_2022,
	title = {Kolmogorov–{Smirnov} test},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Kolmogorov%E2%80%93Smirnov_test&oldid=1122144808},
	abstract = {In statistics, the Kolmogorov–Smirnov test (K-S test or KS test) is a nonparametric test of the equality of continuous (or discontinuous, see Section 2.2), one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test).   In essence, the test answers the question "What is the probability that this collection of samples could have been drawn from that probability distribution?" or, in the second case, "What is the probability that these two sets of samples were drawn from the same (but unknown) probability distribution?".
It is named after  Andrey Kolmogorov and Nikolai Smirnov.
The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case). In the one-sample case, the distribution considered under the null hypothesis may be continuous (see Section 2), purely discrete or mixed (see Section 2.2). In the two-sample case (see Section 3), the distribution considered under the null hypothesis is a continuous distribution but is otherwise unrestricted. However, the two sample test can also be performed under more general conditions that allow for discontinuity, heterogeneity and dependence across samples.The two-sample K–S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples.
The Kolmogorov–Smirnov test can be modified to serve as a goodness of fit test. In the special case of testing for normality of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic (see Test with estimated parameters). Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the Shapiro–Wilk test or Anderson–Darling test. However, these other tests have their own disadvantages. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.},
	language = {en},
	urldate = {2022-11-23},
	journal = {Wikipedia},
	month = nov,
	year = {2022},
	note = {Page Version ID: 1122144808},
	file = {Snapshot:/home/tancrausen/Zotero/storage/TAPTAINW/Kolmogorov–Smirnov_test.html:text/html},
}

@article{tang_confronting_2006,
	title = {Confronting ethnicity-specific disease risk},
	volume = {38},
	number = {1},
	journal = {Nature genetics},
	author = {Tang, Hua},
	year = {2006},
	note = {Publisher: Nature Publishing Group},
	pages = {13--15},
	file = {Full Text:/home/tancrausen/Zotero/storage/DD5464P6/OpenURL.html:text/html;Snapshot:/home/tancrausen/Zotero/storage/AHVVMLV2/ng0106-13.html:text/html},
}

@article{mcquighan_simulating_2010,
	title = {Simulating the poisson process},
	volume = {23},
	journal = {Department of Mathematics-University of Chicago},
	author = {McQuighan, Patrick},
	year = {2010},
	file = {Full Text:/home/tancrausen/Zotero/storage/346DSWCL/McQuighan - 2010 - Simulating the poisson process.pdf:application/pdf},
}

@inproceedings{mclachlan_realistic_2019,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Realistic {Synthetic} {Data} {Generation}: {The} {ATEN} {Framework}},
	isbn = {978-3-030-29196-9},
	shorttitle = {Realistic {Synthetic} {Data} {Generation}},
	doi = {10.1007/978-3-030-29196-9_25},
	abstract = {Getting access to real medical data for research is notoriously difficult. Even when data exist they are usually incomplete and subject to restrictions due to confidentiality and privacy. Synthetic data (SD) are best replacements for real data but must be verifiably realistic. There is little or no investigation into systematically achieving realism in SD. This work investigates this problem, and contributes the ATEN framework, which incorporates three component approaches: (1) THOTH for synthetic data generation (SDG); (2) RA for characterising realism is SD, and (3) HORUS for validating realism in SD. The framework is found promising after its use in generating the realistic synthetic EHR (RS-EHR) for labour and birth. This framework is significant in guaranteeing realism in SDG projects. Future efforts focus on further validation of ATEN in a controlled multi-stream SDG process.},
	language = {en},
	booktitle = {Biomedical {Engineering} {Systems} and {Technologies}},
	publisher = {Springer International Publishing},
	author = {McLachlan, Scott and Dube, Kudakwashe and Gallagher, Thomas and Simmonds, Jennifer A. and Fenton, Norman},
	editor = {Cliquet Jr., Alberto and Wiebe, Sheldon and Anderson, Paul and Saggio, Giovanni and Zwiggelaar, Reyer and Gamboa, Hugo and Fred, Ana and Bermúdez i Badia, Sergi},
	year = {2019},
	keywords = {Knowledge discovery, Synthetic data generation},
	pages = {497--523},
}

@inproceedings{rashidian_smooth-gan_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SMOOTH}-{GAN}: {Towards} {Sharp} and {Smooth} {Synthetic} {EHR} {Data} {Generation}},
	isbn = {978-3-030-59137-3},
	shorttitle = {{SMOOTH}-{GAN}},
	doi = {10.1007/978-3-030-59137-3_4},
	abstract = {Generative adversarial networks (GANs) have been highly successful for generating realistic synthetic data. In healthcare, synthetic data generation can be helpful for producing annotated data and improving data-driven research without worries on data privacy. However, electronic health records (EHRs) are noisy, incomplete and complex, and existing work on EHR data is mainly devoted to generating discrete elements such as diagnosis codes and medications or frequent laboratory values. In this work, we propose SMOOTH-GAN, a novel approach for generating reliable EHR data such as laboratory values and medications given diagnosis codes. SMOOTH-GAN takes advantage of a conditional GAN architecture with WGAN-GP loss, and is able to learn transitions between disease stages with high flexibility over data customization. Our experiments demonstrate the model’s effectiveness in terms of both statistical similarity and accuracy on machine learning based prediction. To further demonstrate the usage of our model, we apply counterfactual reasoning and generate data with occurrence of multiple diseases, which can provide unique datasets for artificial intelligence driven healthcare research.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Medicine}},
	publisher = {Springer International Publishing},
	author = {Rashidian, Sina and Wang, Fusheng and Moffitt, Richard and Garcia, Victor and Dutt, Anurag and Chang, Wei and Pandya, Vishwam and Hajagos, Janos and Saltz, Mary and Saltz, Joel},
	editor = {Michalowski, Martin and Moskovitch, Robert},
	year = {2020},
	keywords = {Generative adversarial networks, Synthetic data generation, Counterfactual machine learning, Electronic health records},
	pages = {37--48},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/AIMN5RFJ/Rashidian et al. - 2020 - SMOOTH-GAN Towards Sharp and Smooth Synthetic EHR.pdf:application/pdf},
}

@inproceedings{dube_approach_2014,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Approach and {Method} for {Generating} {Realistic} {Synthetic} {Electronic} {Healthcare} {Records} for {Secondary} {Use}},
	isbn = {978-3-642-53956-5},
	doi = {10.1007/978-3-642-53956-5_6},
	abstract = {This position paper presents research work involving the development of a publicly available Realistic Synthetic Electronic Healthcare Record (RS-EHR). The paper presents PADARSER, a novel approach in which the real Electronic Healthcare Record (EHR) and neither authorization nor anonymisation are required in generating the synthetic EHR data sets. The GRiSER method is presented for use in PADARSER to allow the RS-EHR to be synthesized for statistically significant localised synthetic patients with statistically prevalent medical conditions based upon information found from publicly available data sources. In treating the synthetic patient within the GRiSER method, clinical workflow or careflows (Cfs) are derived from Clinical Practice Guidelines (CPGs) and the standard local practices of clinicians. The Cfs generated are used together with health statistics, CPGs, medical coding and terminology systems to generate coded synthetic RS-EHR entries from statistically significant observations, treatments, tests, and procedures. The RS-EHR is thus populated with a complete medical history describing the resulting events from treating the medical conditions. The strength of the PADARSER approach is its use of publicly available information. The strengths of the GRiSER method are that (1) it does not require the use of the real EHR for generating the coded RS-EHR entries; and (2) the generic components for obtaining careflow from CPGs and for generating coded RS-EHR entries are applicable in other areas such as knowledge transfer and EHR user interfaces respectively.},
	language = {en},
	booktitle = {Foundations of {Health} {Information} {Engineering} and {Systems}},
	publisher = {Springer},
	author = {Dube, Kudakwashe and Gallagher, Thomas},
	editor = {Gibbons, Jeremy and MacCaull, Wendy},
	year = {2014},
	keywords = {clinical guidelines, clinical workflow, electronic health record, healthcare statistics, ICD10, knowledge modeling, medical terminology, SNOMED-CT, synthetic data},
	pages = {69--86},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/T3NMVMVP/Dube and Gallagher - 2014 - Approach and Method for Generating Realistic Synth.pdf:application/pdf},
}

@article{walonoski_synthea_2018,
	title = {Synthea: {An} approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record},
	volume = {25},
	issn = {1527-974X},
	shorttitle = {Synthea},
	url = {https://doi.org/10.1093/jamia/ocx079},
	doi = {10.1093/jamia/ocx079},
	abstract = {Our objective is to create a source of synthetic electronic health records that is readily available; suited to industrial, innovation, research, and educational uses; and free of legal, privacy, security, and intellectual property restrictions.We developed Synthea, an open-source software package that simulates the lifespans of synthetic patients, modeling the 10 most frequent reasons for primary care encounters and the 10 chronic conditions with the highest morbidity in the United States.Synthea adheres to a previously developed conceptual framework, scales via open-source deployment on the Internet, and may be extended with additional disease and treatment modules developed by its user community. One million synthetic patient records are now freely available online, encoded in standard formats (eg, Health Level-7 [HL7] Fast Healthcare Interoperability Resources [FHIR] and Consolidated-Clinical Document Architecture), and accessible through an HL7 FHIR application program interface.Health care lags other industries in information technology, data exchange, and interoperability. The lack of freely distributable health records has long hindered innovation in health care. Approaches and tools are available to inexpensively generate synthetic health records at scale without accidental disclosure risk, lowering current barriers to entry for promising early-stage developments. By engaging a growing community of users, the synthetic data generated will become increasingly comprehensive, detailed, and realistic over time.Synthetic patients can be simulated with models of disease progression and corresponding standards of care to produce risk-free realistic synthetic health care records at scale.},
	number = {3},
	urldate = {2022-11-24},
	journal = {Journal of the American Medical Informatics Association},
	author = {Walonoski, Jason and Kramer, Mark and Nichols, Joseph and Quina, Andre and Moesel, Chris and Hall, Dylan and Duffett, Carlton and Dube, Kudakwashe and Gallagher, Thomas and McLachlan, Scott},
	month = mar,
	year = {2018},
	pages = {230--238},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/EJ3H9FFS/Walonoski et al. - 2018 - Synthea An approach, method, and software mechani.pdf:application/pdf;Snapshot:/home/tancrausen/Zotero/storage/43KLIMQJ/4098271.html:text/html},
}

@inproceedings{mclachlan_realistic_2019-1,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Realistic {Synthetic} {Data} {Generation}: {The} {ATEN} {Framework}},
	isbn = {978-3-030-29196-9},
	shorttitle = {Realistic {Synthetic} {Data} {Generation}},
	doi = {10.1007/978-3-030-29196-9_25},
	abstract = {Getting access to real medical data for research is notoriously difficult. Even when data exist they are usually incomplete and subject to restrictions due to confidentiality and privacy. Synthetic data (SD) are best replacements for real data but must be verifiably realistic. There is little or no investigation into systematically achieving realism in SD. This work investigates this problem, and contributes the ATEN framework, which incorporates three component approaches: (1) THOTH for synthetic data generation (SDG); (2) RA for characterising realism is SD, and (3) HORUS for validating realism in SD. The framework is found promising after its use in generating the realistic synthetic EHR (RS-EHR) for labour and birth. This framework is significant in guaranteeing realism in SDG projects. Future efforts focus on further validation of ATEN in a controlled multi-stream SDG process.},
	language = {en},
	booktitle = {Biomedical {Engineering} {Systems} and {Technologies}},
	publisher = {Springer International Publishing},
	author = {McLachlan, Scott and Dube, Kudakwashe and Gallagher, Thomas and Simmonds, Jennifer A. and Fenton, Norman},
	editor = {Cliquet Jr., Alberto and Wiebe, Sheldon and Anderson, Paul and Saggio, Giovanni and Zwiggelaar, Reyer and Gamboa, Hugo and Fred, Ana and Bermúdez i Badia, Sergi},
	year = {2019},
	keywords = {Knowledge discovery, Synthetic data generation},
	pages = {497--523},
}

@inproceedings{chin-cheong_generation_2019,
	title = {Generation of heterogeneous synthetic electronic health records using {GANs}},
	booktitle = {workshop on machine learning for health ({ML4H}) at the 33rd conference on neural information processing systems ({NeurIPS} 2019)},
	publisher = {ETH Zurich, Institute for Machine Learning},
	author = {Chin-Cheong, Kieran and Sutter, Thomas and Vogt, Julia E.},
	year = {2019},
	file = {Full Text:/home/tancrausen/Zotero/storage/S8N5KMBL/Chin-Cheong et al. - 2019 - Generation of heterogeneous synthetic electronic h.pdf:application/pdf},
}

@misc{health_national_2012,
	title = {National {Minimum} {Dataset} (hospital events)},
	publisher = {Ministry of Health Wellington},
	author = {Health, Ministry of},
	year = {2012},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	number = {11},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2020},
	note = {Publisher: ACM New York, NY, USA},
	pages = {139--144},
}

@misc{health_national_2012-1,
	title = {National {Minimum} {Dataset} (hospital events)},
	publisher = {Ministry of Health Wellington},
	author = {Health, Ministry of},
	year = {2012},
}

@inproceedings{wang_chestx-ray8_2017-1,
	title = {Chestx-ray8: {Hospital}-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
	shorttitle = {Chestx-ray8},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
	year = {2017},
	pages = {2097--2106},
	file = {Full Text:/home/tancrausen/Zotero/storage/7ZNMPKTJ/Wang et al. - 2017 - Chestx-ray8 Hospital-scale chest x-ray database a.pdf:application/pdf;Snapshot:/home/tancrausen/Zotero/storage/39JFC6XK/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.html:text/html},
}

@article{goodfellow_generative_2020-1,
	title = {Generative adversarial networks},
	volume = {63},
	number = {11},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2020},
	note = {Publisher: ACM New York, NY, USA},
	pages = {139--144},
}

@inproceedings{choi_generating_2017,
	title = {Generating {Multi}-label {Discrete} {Patient} {Records} using {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.mlr.press/v68/choi17a.html},
	abstract = {Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.},
	language = {en},
	urldate = {2022-11-24},
	booktitle = {Proceedings of the 2nd {Machine} {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
	month = nov,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {286--305},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/YLSPJ7BV/Choi et al. - 2017 - Generating Multi-label Discrete Patient Records us.pdf:application/pdf},
}

@inproceedings{rashidian_smooth-gan_2020-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SMOOTH}-{GAN}: {Towards} {Sharp} and {Smooth} {Synthetic} {EHR} {Data} {Generation}},
	isbn = {978-3-030-59137-3},
	shorttitle = {{SMOOTH}-{GAN}},
	doi = {10.1007/978-3-030-59137-3_4},
	abstract = {Generative adversarial networks (GANs) have been highly successful for generating realistic synthetic data. In healthcare, synthetic data generation can be helpful for producing annotated data and improving data-driven research without worries on data privacy. However, electronic health records (EHRs) are noisy, incomplete and complex, and existing work on EHR data is mainly devoted to generating discrete elements such as diagnosis codes and medications or frequent laboratory values. In this work, we propose SMOOTH-GAN, a novel approach for generating reliable EHR data such as laboratory values and medications given diagnosis codes. SMOOTH-GAN takes advantage of a conditional GAN architecture with WGAN-GP loss, and is able to learn transitions between disease stages with high flexibility over data customization. Our experiments demonstrate the model’s effectiveness in terms of both statistical similarity and accuracy on machine learning based prediction. To further demonstrate the usage of our model, we apply counterfactual reasoning and generate data with occurrence of multiple diseases, which can provide unique datasets for artificial intelligence driven healthcare research.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Medicine}},
	publisher = {Springer International Publishing},
	author = {Rashidian, Sina and Wang, Fusheng and Moffitt, Richard and Garcia, Victor and Dutt, Anurag and Chang, Wei and Pandya, Vishwam and Hajagos, Janos and Saltz, Mary and Saltz, Joel},
	editor = {Michalowski, Martin and Moskovitch, Robert},
	year = {2020},
	keywords = {Generative adversarial networks, Synthetic data generation, Counterfactual machine learning, Electronic health records},
	pages = {37--48},
	file = {Full Text PDF:/home/tancrausen/Zotero/storage/SR6KQH4T/Rashidian et al. - 2020 - SMOOTH-GAN Towards Sharp and Smooth Synthetic EHR.pdf:application/pdf},
}

@article{walonoski_synthea_2018-1,
	title = {Synthea: {An} approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record},
	volume = {25},
	shorttitle = {Synthea},
	number = {3},
	journal = {Journal of the American Medical Informatics Association},
	author = {Walonoski, Jason and Kramer, Mark and Nichols, Joseph and Quina, Andre and Moesel, Chris and Hall, Dylan and Duffett, Carlton and Dube, Kudakwashe and Gallagher, Thomas and McLachlan, Scott},
	year = {2018},
	note = {Publisher: Oxford University Press},
	pages = {230--238},
	file = {Full Text:/home/tancrausen/Zotero/storage/DNIV2N54/4098271.html:text/html;Snapshot:/home/tancrausen/Zotero/storage/E5WZ3BFH/4098271.html:text/html},
}

@article{roberts_characterization_1971,
	title = {A characterization of clique graphs},
	volume = {10},
	number = {2},
	journal = {Journal of Combinatorial Theory, Series B},
	author = {Roberts, Fred S. and Spencer, Joel H.},
	year = {1971},
	note = {Publisher: Elsevier},
	pages = {102--108},
	file = {Full Text:/home/tancrausen/Zotero/storage/REUF8QXJ/Roberts and Spencer - 1971 - A characterization of clique graphs.pdf:application/pdf},
}

@article{bertoli_overview_2007,
	title = {An overview of the jmt queueing network simulator},
	volume = {2007},
	journal = {Politecnico di Milano-DEI, Tech. Rep. TR},
	author = {Bertoli, Marco and Casale, Giuliano and Serazzi, Guiseppe},
	year = {2007},
	note = {Publisher: Citeseer},
	file = {Full Text:/home/tancrausen/Zotero/storage/NYMFX4FZ/Bertoli et al. - 2007 - An overview of the jmt queueing network simulator.pdf:application/pdf},
}

@article{cappiello_enabling_2022,
	title = {Enabling {Real}-world {Medicine} with {Data} {Lake} {Federation}: a research perspective},
	journal = {The Eighth International Workshop on Data Management and Analytics for Medicine and Healthcare (DMAH)},
	author = {Cappiello, Cinzia and Gribaudo, Marco and Plebani, Pierluigi and Salnitri, Mattia and Tanca, Letizia},
	year = {2022},
	annote = {[accepted for publication]},
}

@misc{johnson_mimic-iv_2022,
	title = {{MIMIC}-{IV}},
	url = {https://physionet.org/content/mimiciv/2.0/},
	abstract = {Retrospectively collected medical data has the opportunity to improve patient
care through knowledge discovery and algorithm development. Broad reuse of
medical data is desirable for the greatest public good, but data sharing must
be done in a manner which protects patient privacy. The Medical Information
Mart for Intensive Care (MIMIC)-III database provided critical care data for
over 40,000 patients admitted to intensive care units at the Beth Israel
Deaconess Medical Center (BIDMC). Importantly, MIMIC-III was deidentified, and
patient identifiers were removed according to the Health Insurance Portability
and Accountability Act (HIPAA) Safe Harbor provision. MIMIC-III has been
integral in driving large amounts of research in clinical informatics,
epidemiology, and machine learning. Here we present MIMIC-IV, an update to
MIMIC-III, which incorporates contemporary data and improves on numerous
aspects of MIMIC-III. MIMIC-IV adopts a modular approach to data organization,
highlighting data provenance and facilitating both individual and combined use
of disparate data sources. MIMIC-IV is intended to carry on the success of
MIMIC-III and support a broad set of applications within healthcare.},
	urldate = {2022-11-29},
	publisher = {PhysioNet},
	author = {Johnson, Alistair and Bulgarelli, Lucas and Pollard, Tom and Horng, Steven and Celi, Leo Anthony and Mark, Roger},
	year = {2022},
	doi = {10.13026/7VCR-E114},
	note = {Version Number: 2.0
Type: dataset},
}
